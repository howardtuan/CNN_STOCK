{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "from zipfile import ZipFile\n",
    "from importlib import reload\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from contextlib import contextmanager\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c2d292fe90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 參數設置\n",
    "use_gpu = True\n",
    "use_random_split = False\n",
    "use_dataparallel = True\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 圖片維度常數\n",
    "IMG_HEIGHT = {5: 32, 20: 64, 60: 96}\n",
    "IMG_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMG_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用裝置: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 檢查 GPU 是否可用\n",
    "if use_gpu:\n",
    "    def query_gpu(qargs=[]):\n",
    "        qargs = ['index', 'gpu_name', 'memory.free'] + qargs\n",
    "        cmd = 'nvidia-smi --query-gpu={} --format=csv,noheader'.format(','.join(qargs))\n",
    "        results = os.popen(cmd).readlines()\n",
    "        return results\n",
    "\n",
    "    def select_gpu(results, thres=4096):\n",
    "        available = []\n",
    "        try:\n",
    "            for i, line in enumerate(results):\n",
    "                if int(re.findall(r'(.*), (.*?) MiB', line)[0][-1]) > thres:\n",
    "                    available.append(i)\n",
    "            return available\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([str(gpu) for gpu in select_gpu(query_gpu())])\n",
    "    \n",
    "if not torch.cuda.is_available() and use_gpu:\n",
    "    raise RuntimeError(\"此程式需要 GPU 才能運行！請確認 CUDA 環境是否正確安裝。\")\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "print(f\"使用裝置: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.labels = pd.read_excel(labels_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((IMG_HEIGHT[5], IMG_WIDTH[5])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     img_path = os.path.join(self.img_dir, self.labels.iloc[idx, 0])\n",
    "    #     try:\n",
    "    #         image = Image.open(img_path).convert('1')  # Convert to grayscale\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error opening image {img_path}: {e}\")\n",
    "    #         image = Image.new('1', (IMG_WIDTH_SELECTED, IMG_HEIGHT_SELECTED), 0)  # Return a blank image\n",
    "    #     image = self.transform(image) if self.transform else image\n",
    "    #     label = int(self.labels.iloc[idx, 1])\n",
    "    #     return image, label\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels.iloc[idx, 0])\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            for ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:\n",
    "                test_path = img_path + ext\n",
    "                if os.path.exists(test_path):\n",
    "                    img_path = test_path\n",
    "                    break\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('1')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            image = Image.new('1', (IMG_WIDTH, IMG_HEIGHT), 0)\n",
    "        \n",
    "        if self.transform:\n",
    "            try:\n",
    "                image = self.transform(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error transforming image {img_path}: {e}\")\n",
    "                blank = Image.new('1', (IMG_WIDTH, IMG_HEIGHT), 0)\n",
    "                image = self.transform(blank)\n",
    "        \n",
    "        label = int(self.labels.iloc[idx, 1])\n",
    "        return image, label\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     img_path = os.path.join(self.img_dir, self.labels.iloc[idx, 0])\n",
    "        \n",
    "    #     if not os.path.exists(img_path):\n",
    "    #         img_path = next((img_path + ext for ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff'] if os.path.exists(img_path + ext)), None)\n",
    "        \n",
    "    #     image = Image.open(img_path).convert('1') if img_path else Image.new('1', (IMG_WIDTH[5], IMG_HEIGHT[5]), 0)\n",
    "    #     image = self.transform(image) if self.transform else image\n",
    "    #     label = int(self.labels.iloc[idx, 1])\n",
    "        \n",
    "    #     return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT_SELECTED = 32\n",
    "IMG_WIDTH_SELECTED = 15\n",
    "\n",
    "class CNN5d(nn.Module):\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)  # 修正\n",
    "            m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN5d, self).__init__()\n",
    "        self.conv1 = nn.Sequential(OrderedDict([\n",
    "            ('Conv', nn.Conv2d(1, 64, (5, 3), padding=(2, 1), stride=(1, 1), dilation=(1, 1))),\n",
    "            ('BN', nn.BatchNorm2d(64, affine=True)),\n",
    "            ('ReLU', nn.ReLU()),\n",
    "            ('Max-Pool', nn.MaxPool2d((2, 1)))\n",
    "        ]))\n",
    "        self.conv1 = self.conv1.apply(self.init_weights)\n",
    "        \n",
    "        self.conv2 = nn.Sequential(OrderedDict([\n",
    "            ('Conv', nn.Conv2d(64, 128, (5, 3), padding=(2, 1), stride=(1, 1), dilation=(1, 1))),\n",
    "            ('BN', nn.BatchNorm2d(128, affine=True)),\n",
    "            ('ReLU', nn.ReLU()),\n",
    "            ('Max-Pool', nn.MaxPool2d((2, 1)))\n",
    "        ]))\n",
    "        self.conv2 = self.conv2.apply(self.init_weights)\n",
    "        \n",
    "        # 計算展平大小\n",
    "        dummy_input = torch.zeros(1, 1, IMG_HEIGHT_SELECTED, IMG_WIDTH_SELECTED)\n",
    "        flattened_size = self.conv2(self.conv1(dummy_input)).view(1, -1).shape[1]\n",
    "\n",
    "        self.DropOut = nn.Dropout(p=0.5)\n",
    "        self.FC = nn.Linear(flattened_size, 2)\n",
    "        self.FC.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x): \n",
    "        # 輸入數據應為 [N, 32, 15]\n",
    "        if x.ndim == 4:  # 若有多餘維度，去掉\n",
    "            x = x.squeeze(1)\n",
    "        x = x.unsqueeze(1).to(torch.float32)  # 增加通道維度，變為 [N, 1, 32, 15]\n",
    "        x = self.conv1(x)  # 通過卷積層\n",
    "        x = self.conv2(x)  # 通過第二層卷積\n",
    "        x = self.DropOut(x.view(x.shape[0], -1))  # 展平\n",
    "        x = self.FC(x)  # 全連接層\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_HEIGHT_SELECTED = 32\n",
    "# IMG_WIDTH_SELECTED = 15\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, 64, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1)),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d((2, 1), stride=(2, 1))\n",
    "#         )\n",
    "        \n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 128, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1)),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d((2, 1), stride=(2, 1))\n",
    "#         )\n",
    "        \n",
    "#         # Calculate the flattened size before FC layer\n",
    "#         self.fc = nn.Linear(128 * (IMG_HEIGHT_SELECTED // 4) * IMG_WIDTH_SELECTED, 15360)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 1, IMG_HEIGHT_SELECTED, IMG_WIDTH_SELECTED)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten\n",
    "#         x = self.fc(x)\n",
    "#         x = self.softmax(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old model\n",
    "\n",
    "# IMG_HEIGHT_SELECTED = 32  # Set to the fixed height of your image format\n",
    "# IMG_WIDTH_SELECTED = 15   # Set to the fixed width of your image format\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#             nn.LayerNorm([32, IMG_HEIGHT_SELECTED, IMG_WIDTH_SELECTED]),\n",
    "#             nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "#         )\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#             nn.LayerNorm([64, IMG_HEIGHT_SELECTED // 2, IMG_WIDTH_SELECTED // 2]),\n",
    "#             nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "#             nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "#         )\n",
    "#         # Update the linear layer to match the reduced size after pooling\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(64 * (IMG_HEIGHT_SELECTED // 4) * (IMG_WIDTH_SELECTED // 4), 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 1, IMG_HEIGHT_SELECTED, IMG_WIDTH_SELECTED)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten for the fully connected layer\n",
    "#         return self.fc1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=30):\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    best_test_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                test_loss += criterion(outputs, labels).item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        test_loss /= len(test_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print(f'GPU 記憶體使用: {torch.cuda.memory_allocated(DEVICE) / 1024**2:.1f}MB')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "        \n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': test_loss,\n",
    "            }, 'best_model.pth')\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Class 0', 'Class 1']))\n",
    "    \n",
    "    return accuracy, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在載入數據...\n",
      "初始化模型...\n",
      "開始訓練...\n",
      "Epoch [1/10], Batch [0/20508], Loss: 1.8046\n",
      "Epoch [1/10], Batch [10/20508], Loss: 1.2332\n",
      "Epoch [1/10], Batch [20/20508], Loss: 1.1095\n",
      "Epoch [1/10], Batch [30/20508], Loss: 1.1553\n",
      "Epoch [1/10], Batch [40/20508], Loss: 1.0263\n",
      "Epoch [1/10], Batch [50/20508], Loss: 0.9939\n",
      "Epoch [1/10], Batch [60/20508], Loss: 1.0370\n",
      "Epoch [1/10], Batch [70/20508], Loss: 0.9212\n",
      "Epoch [1/10], Batch [80/20508], Loss: 1.0727\n",
      "Epoch [1/10], Batch [90/20508], Loss: 1.0174\n",
      "Epoch [1/10], Batch [100/20508], Loss: 0.9905\n",
      "Epoch [1/10], Batch [110/20508], Loss: 1.0257\n",
      "Epoch [1/10], Batch [120/20508], Loss: 0.9981\n",
      "Epoch [1/10], Batch [130/20508], Loss: 1.1537\n",
      "Epoch [1/10], Batch [140/20508], Loss: 0.9738\n",
      "Epoch [1/10], Batch [150/20508], Loss: 0.9750\n",
      "Epoch [1/10], Batch [160/20508], Loss: 1.0172\n",
      "Epoch [1/10], Batch [170/20508], Loss: 1.1036\n",
      "Epoch [1/10], Batch [180/20508], Loss: 0.8635\n",
      "Epoch [1/10], Batch [190/20508], Loss: 1.0031\n",
      "Epoch [1/10], Batch [200/20508], Loss: 0.9749\n",
      "Epoch [1/10], Batch [210/20508], Loss: 1.0291\n",
      "Epoch [1/10], Batch [220/20508], Loss: 1.1048\n",
      "Epoch [1/10], Batch [230/20508], Loss: 1.0105\n",
      "Epoch [1/10], Batch [240/20508], Loss: 1.0453\n",
      "Epoch [1/10], Batch [250/20508], Loss: 1.0472\n",
      "Epoch [1/10], Batch [260/20508], Loss: 1.1101\n",
      "Epoch [1/10], Batch [270/20508], Loss: 1.0204\n",
      "Epoch [1/10], Batch [280/20508], Loss: 1.0028\n",
      "Epoch [1/10], Batch [290/20508], Loss: 0.9415\n",
      "Epoch [1/10], Batch [300/20508], Loss: 1.0923\n",
      "Epoch [1/10], Batch [310/20508], Loss: 1.0853\n",
      "Epoch [1/10], Batch [320/20508], Loss: 1.1345\n",
      "Epoch [1/10], Batch [330/20508], Loss: 0.9982\n",
      "Epoch [1/10], Batch [340/20508], Loss: 1.0315\n",
      "Epoch [1/10], Batch [350/20508], Loss: 0.9007\n",
      "Epoch [1/10], Batch [360/20508], Loss: 1.1356\n",
      "Epoch [1/10], Batch [370/20508], Loss: 0.9722\n",
      "Epoch [1/10], Batch [380/20508], Loss: 0.9582\n",
      "Epoch [1/10], Batch [390/20508], Loss: 1.0958\n",
      "Epoch [1/10], Batch [400/20508], Loss: 0.9188\n",
      "Epoch [1/10], Batch [410/20508], Loss: 0.8926\n",
      "Epoch [1/10], Batch [420/20508], Loss: 0.7858\n",
      "Epoch [1/10], Batch [430/20508], Loss: 0.9727\n",
      "Epoch [1/10], Batch [440/20508], Loss: 0.9273\n",
      "Epoch [1/10], Batch [450/20508], Loss: 0.9156\n",
      "Epoch [1/10], Batch [460/20508], Loss: 1.0843\n",
      "Epoch [1/10], Batch [470/20508], Loss: 1.0003\n",
      "Epoch [1/10], Batch [480/20508], Loss: 1.0162\n",
      "Epoch [1/10], Batch [490/20508], Loss: 1.1449\n",
      "Epoch [1/10], Batch [500/20508], Loss: 0.8646\n",
      "Epoch [1/10], Batch [510/20508], Loss: 0.9527\n",
      "Epoch [1/10], Batch [520/20508], Loss: 0.8699\n",
      "Epoch [1/10], Batch [530/20508], Loss: 0.8642\n",
      "Epoch [1/10], Batch [540/20508], Loss: 0.8779\n",
      "Epoch [1/10], Batch [550/20508], Loss: 0.9375\n",
      "Epoch [1/10], Batch [560/20508], Loss: 1.1331\n",
      "Epoch [1/10], Batch [570/20508], Loss: 0.9205\n",
      "Epoch [1/10], Batch [580/20508], Loss: 0.9162\n",
      "Epoch [1/10], Batch [590/20508], Loss: 1.1208\n",
      "Epoch [1/10], Batch [600/20508], Loss: 1.0244\n",
      "Epoch [1/10], Batch [610/20508], Loss: 1.0314\n",
      "Epoch [1/10], Batch [620/20508], Loss: 0.8862\n",
      "Epoch [1/10], Batch [630/20508], Loss: 1.0679\n",
      "Epoch [1/10], Batch [640/20508], Loss: 1.1527\n",
      "Epoch [1/10], Batch [650/20508], Loss: 1.0247\n",
      "Epoch [1/10], Batch [660/20508], Loss: 1.0223\n",
      "Epoch [1/10], Batch [670/20508], Loss: 1.0227\n",
      "Epoch [1/10], Batch [680/20508], Loss: 1.1453\n",
      "Epoch [1/10], Batch [690/20508], Loss: 1.0889\n",
      "Epoch [1/10], Batch [700/20508], Loss: 1.1274\n",
      "Epoch [1/10], Batch [710/20508], Loss: 1.1218\n",
      "Epoch [1/10], Batch [720/20508], Loss: 0.8220\n",
      "Epoch [1/10], Batch [730/20508], Loss: 0.9657\n",
      "Epoch [1/10], Batch [740/20508], Loss: 0.9302\n",
      "Epoch [1/10], Batch [750/20508], Loss: 0.9261\n",
      "Epoch [1/10], Batch [760/20508], Loss: 0.9841\n",
      "Epoch [1/10], Batch [770/20508], Loss: 1.1310\n",
      "Epoch [1/10], Batch [780/20508], Loss: 1.0105\n",
      "Epoch [1/10], Batch [790/20508], Loss: 0.9227\n",
      "Epoch [1/10], Batch [800/20508], Loss: 1.1093\n",
      "Epoch [1/10], Batch [810/20508], Loss: 0.8542\n",
      "Epoch [1/10], Batch [820/20508], Loss: 0.9219\n",
      "Epoch [1/10], Batch [830/20508], Loss: 0.8393\n",
      "Epoch [1/10], Batch [840/20508], Loss: 0.8336\n",
      "Epoch [1/10], Batch [850/20508], Loss: 0.9971\n",
      "Epoch [1/10], Batch [860/20508], Loss: 0.9343\n",
      "Epoch [1/10], Batch [870/20508], Loss: 0.9634\n",
      "Epoch [1/10], Batch [880/20508], Loss: 1.0299\n",
      "Epoch [1/10], Batch [890/20508], Loss: 1.0091\n",
      "Epoch [1/10], Batch [900/20508], Loss: 0.8909\n",
      "Epoch [1/10], Batch [910/20508], Loss: 1.0288\n",
      "Epoch [1/10], Batch [920/20508], Loss: 0.9661\n",
      "Epoch [1/10], Batch [930/20508], Loss: 1.0124\n",
      "Epoch [1/10], Batch [940/20508], Loss: 0.9775\n",
      "Epoch [1/10], Batch [950/20508], Loss: 1.0296\n",
      "Epoch [1/10], Batch [960/20508], Loss: 1.0419\n",
      "Epoch [1/10], Batch [970/20508], Loss: 0.9780\n",
      "Epoch [1/10], Batch [980/20508], Loss: 1.0207\n",
      "Epoch [1/10], Batch [990/20508], Loss: 0.8742\n",
      "Epoch [1/10], Batch [1000/20508], Loss: 0.9003\n",
      "Epoch [1/10], Batch [1010/20508], Loss: 0.9264\n",
      "Epoch [1/10], Batch [1020/20508], Loss: 0.8929\n",
      "Epoch [1/10], Batch [1030/20508], Loss: 0.9292\n",
      "Epoch [1/10], Batch [1040/20508], Loss: 1.0047\n",
      "Epoch [1/10], Batch [1050/20508], Loss: 0.8489\n",
      "Epoch [1/10], Batch [1060/20508], Loss: 1.0307\n",
      "Epoch [1/10], Batch [1070/20508], Loss: 0.9661\n",
      "Epoch [1/10], Batch [1080/20508], Loss: 1.0114\n",
      "Epoch [1/10], Batch [1090/20508], Loss: 0.9115\n",
      "Epoch [1/10], Batch [1100/20508], Loss: 0.9714\n",
      "Epoch [1/10], Batch [1110/20508], Loss: 1.0378\n",
      "Epoch [1/10], Batch [1120/20508], Loss: 0.9670\n",
      "Epoch [1/10], Batch [1130/20508], Loss: 0.8640\n",
      "Epoch [1/10], Batch [1140/20508], Loss: 0.8036\n",
      "Epoch [1/10], Batch [1150/20508], Loss: 0.9881\n",
      "Epoch [1/10], Batch [1160/20508], Loss: 0.9402\n",
      "Epoch [1/10], Batch [1170/20508], Loss: 0.9547\n",
      "Epoch [1/10], Batch [1180/20508], Loss: 0.9734\n",
      "Epoch [1/10], Batch [1190/20508], Loss: 1.1191\n",
      "Epoch [1/10], Batch [1200/20508], Loss: 0.8802\n",
      "Epoch [1/10], Batch [1210/20508], Loss: 1.0494\n",
      "Epoch [1/10], Batch [1220/20508], Loss: 0.9103\n",
      "Epoch [1/10], Batch [1230/20508], Loss: 1.0425\n",
      "Epoch [1/10], Batch [1240/20508], Loss: 0.9496\n",
      "Epoch [1/10], Batch [1250/20508], Loss: 0.8754\n",
      "Epoch [1/10], Batch [1260/20508], Loss: 1.1032\n",
      "Epoch [1/10], Batch [1270/20508], Loss: 0.9728\n",
      "Epoch [1/10], Batch [1280/20508], Loss: 0.9801\n",
      "Epoch [1/10], Batch [1290/20508], Loss: 0.9103\n",
      "Epoch [1/10], Batch [1300/20508], Loss: 0.9615\n",
      "Epoch [1/10], Batch [1310/20508], Loss: 0.9330\n",
      "Epoch [1/10], Batch [1320/20508], Loss: 0.9025\n",
      "Epoch [1/10], Batch [1330/20508], Loss: 1.0494\n",
      "Epoch [1/10], Batch [1340/20508], Loss: 0.9105\n",
      "Epoch [1/10], Batch [1350/20508], Loss: 1.0763\n",
      "Epoch [1/10], Batch [1360/20508], Loss: 0.9134\n",
      "Epoch [1/10], Batch [1370/20508], Loss: 0.9452\n",
      "Epoch [1/10], Batch [1380/20508], Loss: 0.9879\n",
      "Epoch [1/10], Batch [1390/20508], Loss: 1.0078\n",
      "Epoch [1/10], Batch [1400/20508], Loss: 1.0397\n",
      "Epoch [1/10], Batch [1410/20508], Loss: 0.9842\n",
      "Epoch [1/10], Batch [1420/20508], Loss: 0.9375\n",
      "Epoch [1/10], Batch [1430/20508], Loss: 0.8447\n",
      "Epoch [1/10], Batch [1440/20508], Loss: 0.9805\n",
      "Epoch [1/10], Batch [1450/20508], Loss: 0.8344\n",
      "Epoch [1/10], Batch [1460/20508], Loss: 0.9987\n",
      "Epoch [1/10], Batch [1470/20508], Loss: 0.9315\n",
      "Epoch [1/10], Batch [1480/20508], Loss: 0.9315\n",
      "Epoch [1/10], Batch [1490/20508], Loss: 0.8665\n",
      "Epoch [1/10], Batch [1500/20508], Loss: 0.8964\n",
      "Epoch [1/10], Batch [1510/20508], Loss: 0.9365\n",
      "Epoch [1/10], Batch [1520/20508], Loss: 0.9463\n",
      "Epoch [1/10], Batch [1530/20508], Loss: 0.8534\n",
      "Epoch [1/10], Batch [1540/20508], Loss: 0.9434\n",
      "Epoch [1/10], Batch [1550/20508], Loss: 0.8655\n",
      "Epoch [1/10], Batch [1560/20508], Loss: 0.9916\n",
      "Epoch [1/10], Batch [1570/20508], Loss: 1.0473\n",
      "Epoch [1/10], Batch [1580/20508], Loss: 0.9618\n",
      "Epoch [1/10], Batch [1590/20508], Loss: 0.8235\n",
      "Epoch [1/10], Batch [1600/20508], Loss: 0.9714\n",
      "Epoch [1/10], Batch [1610/20508], Loss: 0.8936\n",
      "Epoch [1/10], Batch [1620/20508], Loss: 0.8576\n",
      "Epoch [1/10], Batch [1630/20508], Loss: 0.9698\n",
      "Epoch [1/10], Batch [1640/20508], Loss: 0.9130\n",
      "Epoch [1/10], Batch [1650/20508], Loss: 0.9391\n",
      "Epoch [1/10], Batch [1660/20508], Loss: 1.0227\n",
      "Epoch [1/10], Batch [1670/20508], Loss: 0.8579\n",
      "Epoch [1/10], Batch [1680/20508], Loss: 0.7969\n",
      "Epoch [1/10], Batch [1690/20508], Loss: 0.9683\n",
      "Epoch [1/10], Batch [1700/20508], Loss: 0.8876\n",
      "Epoch [1/10], Batch [1710/20508], Loss: 0.9238\n",
      "Epoch [1/10], Batch [1720/20508], Loss: 0.8595\n",
      "Epoch [1/10], Batch [1730/20508], Loss: 0.9086\n",
      "Epoch [1/10], Batch [1740/20508], Loss: 0.9236\n",
      "Epoch [1/10], Batch [1750/20508], Loss: 0.8139\n",
      "Epoch [1/10], Batch [1760/20508], Loss: 0.9232\n",
      "Epoch [1/10], Batch [1770/20508], Loss: 0.9103\n",
      "Epoch [1/10], Batch [1780/20508], Loss: 0.9173\n",
      "Epoch [1/10], Batch [1790/20508], Loss: 0.9596\n",
      "Epoch [1/10], Batch [1800/20508], Loss: 0.8487\n",
      "Epoch [1/10], Batch [1810/20508], Loss: 0.9968\n",
      "Epoch [1/10], Batch [1820/20508], Loss: 0.9572\n",
      "Epoch [1/10], Batch [1830/20508], Loss: 1.0159\n",
      "Epoch [1/10], Batch [1840/20508], Loss: 0.8042\n",
      "Epoch [1/10], Batch [1850/20508], Loss: 0.9446\n",
      "Epoch [1/10], Batch [1860/20508], Loss: 0.8966\n",
      "Epoch [1/10], Batch [1870/20508], Loss: 1.0138\n",
      "Epoch [1/10], Batch [1880/20508], Loss: 0.8311\n",
      "Epoch [1/10], Batch [1890/20508], Loss: 0.9179\n",
      "Epoch [1/10], Batch [1900/20508], Loss: 0.9780\n",
      "Epoch [1/10], Batch [1910/20508], Loss: 0.9212\n",
      "Epoch [1/10], Batch [1920/20508], Loss: 0.8097\n",
      "Epoch [1/10], Batch [1930/20508], Loss: 0.9173\n",
      "Epoch [1/10], Batch [1940/20508], Loss: 0.8771\n",
      "Epoch [1/10], Batch [1950/20508], Loss: 0.8516\n",
      "Epoch [1/10], Batch [1960/20508], Loss: 0.8129\n",
      "Epoch [1/10], Batch [1970/20508], Loss: 0.9255\n",
      "Epoch [1/10], Batch [1980/20508], Loss: 0.9007\n",
      "Epoch [1/10], Batch [1990/20508], Loss: 0.9747\n",
      "Epoch [1/10], Batch [2000/20508], Loss: 0.8872\n",
      "Epoch [1/10], Batch [2010/20508], Loss: 0.8767\n",
      "Epoch [1/10], Batch [2020/20508], Loss: 1.0456\n",
      "Epoch [1/10], Batch [2030/20508], Loss: 0.8603\n",
      "Epoch [1/10], Batch [2040/20508], Loss: 0.9793\n",
      "Epoch [1/10], Batch [2050/20508], Loss: 0.8096\n",
      "Epoch [1/10], Batch [2060/20508], Loss: 0.9383\n",
      "Epoch [1/10], Batch [2070/20508], Loss: 0.9375\n",
      "Epoch [1/10], Batch [2080/20508], Loss: 0.8640\n",
      "Epoch [1/10], Batch [2090/20508], Loss: 0.9210\n",
      "Epoch [1/10], Batch [2100/20508], Loss: 0.9823\n",
      "Epoch [1/10], Batch [2110/20508], Loss: 0.9298\n",
      "Epoch [1/10], Batch [2120/20508], Loss: 0.9216\n",
      "Epoch [1/10], Batch [2130/20508], Loss: 0.8435\n",
      "Epoch [1/10], Batch [2140/20508], Loss: 0.8408\n",
      "Epoch [1/10], Batch [2150/20508], Loss: 0.8882\n",
      "Epoch [1/10], Batch [2160/20508], Loss: 0.8166\n",
      "Epoch [1/10], Batch [2170/20508], Loss: 0.8678\n",
      "Epoch [1/10], Batch [2180/20508], Loss: 0.9928\n",
      "Epoch [1/10], Batch [2190/20508], Loss: 0.9407\n",
      "Epoch [1/10], Batch [2200/20508], Loss: 0.9554\n",
      "Epoch [1/10], Batch [2210/20508], Loss: 0.8391\n",
      "Epoch [1/10], Batch [2220/20508], Loss: 0.9032\n",
      "Epoch [1/10], Batch [2230/20508], Loss: 0.9155\n",
      "Epoch [1/10], Batch [2240/20508], Loss: 0.8475\n",
      "Epoch [1/10], Batch [2250/20508], Loss: 0.8680\n",
      "Epoch [1/10], Batch [2260/20508], Loss: 0.8831\n",
      "Epoch [1/10], Batch [2270/20508], Loss: 0.9710\n",
      "Epoch [1/10], Batch [2280/20508], Loss: 0.9844\n",
      "Epoch [1/10], Batch [2290/20508], Loss: 0.8368\n",
      "Epoch [1/10], Batch [2300/20508], Loss: 0.8679\n",
      "Epoch [1/10], Batch [2310/20508], Loss: 0.9096\n",
      "Epoch [1/10], Batch [2320/20508], Loss: 0.8528\n",
      "Epoch [1/10], Batch [2330/20508], Loss: 0.8406\n",
      "Epoch [1/10], Batch [2340/20508], Loss: 0.9052\n",
      "Epoch [1/10], Batch [2350/20508], Loss: 0.8598\n",
      "Epoch [1/10], Batch [2360/20508], Loss: 0.8829\n",
      "Epoch [1/10], Batch [2370/20508], Loss: 0.8741\n",
      "Epoch [1/10], Batch [2380/20508], Loss: 0.9739\n",
      "Epoch [1/10], Batch [2390/20508], Loss: 0.9329\n",
      "Epoch [1/10], Batch [2400/20508], Loss: 0.8591\n",
      "Epoch [1/10], Batch [2410/20508], Loss: 0.9396\n",
      "Epoch [1/10], Batch [2420/20508], Loss: 0.8365\n",
      "Epoch [1/10], Batch [2430/20508], Loss: 0.9712\n",
      "Epoch [1/10], Batch [2440/20508], Loss: 0.9120\n",
      "Epoch [1/10], Batch [2450/20508], Loss: 0.9127\n",
      "Epoch [1/10], Batch [2460/20508], Loss: 0.9110\n",
      "Epoch [1/10], Batch [2470/20508], Loss: 0.8501\n",
      "Epoch [1/10], Batch [2480/20508], Loss: 0.8912\n",
      "Epoch [1/10], Batch [2490/20508], Loss: 0.9187\n",
      "Epoch [1/10], Batch [2500/20508], Loss: 0.9108\n",
      "Epoch [1/10], Batch [2510/20508], Loss: 0.9445\n",
      "Epoch [1/10], Batch [2520/20508], Loss: 0.8593\n",
      "Epoch [1/10], Batch [2530/20508], Loss: 0.8650\n",
      "Epoch [1/10], Batch [2540/20508], Loss: 0.8377\n",
      "Epoch [1/10], Batch [2550/20508], Loss: 0.8855\n",
      "Epoch [1/10], Batch [2560/20508], Loss: 0.8306\n",
      "Epoch [1/10], Batch [2570/20508], Loss: 1.0176\n",
      "Epoch [1/10], Batch [2580/20508], Loss: 0.8668\n",
      "Epoch [1/10], Batch [2590/20508], Loss: 0.8430\n",
      "Epoch [1/10], Batch [2600/20508], Loss: 0.8877\n",
      "Epoch [1/10], Batch [2610/20508], Loss: 0.8988\n",
      "Epoch [1/10], Batch [2620/20508], Loss: 0.9177\n",
      "Epoch [1/10], Batch [2630/20508], Loss: 0.9010\n",
      "Epoch [1/10], Batch [2640/20508], Loss: 0.9515\n",
      "Epoch [1/10], Batch [2650/20508], Loss: 0.8684\n",
      "Epoch [1/10], Batch [2660/20508], Loss: 0.8575\n",
      "Epoch [1/10], Batch [2670/20508], Loss: 0.9061\n",
      "Epoch [1/10], Batch [2680/20508], Loss: 0.8493\n",
      "Epoch [1/10], Batch [2690/20508], Loss: 0.8865\n",
      "Epoch [1/10], Batch [2700/20508], Loss: 0.8973\n",
      "Epoch [1/10], Batch [2710/20508], Loss: 0.8857\n",
      "Epoch [1/10], Batch [2720/20508], Loss: 0.8873\n",
      "Epoch [1/10], Batch [2730/20508], Loss: 0.7790\n",
      "Epoch [1/10], Batch [2740/20508], Loss: 0.8759\n",
      "Epoch [1/10], Batch [2750/20508], Loss: 0.9007\n",
      "Epoch [1/10], Batch [2760/20508], Loss: 0.9428\n",
      "Epoch [1/10], Batch [2770/20508], Loss: 0.8789\n",
      "Epoch [1/10], Batch [2780/20508], Loss: 0.8459\n",
      "Epoch [1/10], Batch [2790/20508], Loss: 0.9857\n",
      "Epoch [1/10], Batch [2800/20508], Loss: 0.8613\n",
      "Epoch [1/10], Batch [2810/20508], Loss: 0.9275\n",
      "Epoch [1/10], Batch [2820/20508], Loss: 0.8473\n",
      "Epoch [1/10], Batch [2830/20508], Loss: 0.9055\n",
      "Epoch [1/10], Batch [2840/20508], Loss: 0.9095\n",
      "Epoch [1/10], Batch [2850/20508], Loss: 0.8318\n",
      "Epoch [1/10], Batch [2860/20508], Loss: 0.8080\n",
      "Epoch [1/10], Batch [2870/20508], Loss: 0.8112\n",
      "Epoch [1/10], Batch [2880/20508], Loss: 0.8503\n",
      "Epoch [1/10], Batch [2890/20508], Loss: 0.7368\n",
      "Epoch [1/10], Batch [2900/20508], Loss: 0.8384\n",
      "Epoch [1/10], Batch [2910/20508], Loss: 0.8834\n",
      "Epoch [1/10], Batch [2920/20508], Loss: 0.8839\n",
      "Epoch [1/10], Batch [2930/20508], Loss: 0.8950\n",
      "Epoch [1/10], Batch [2940/20508], Loss: 0.9175\n",
      "Epoch [1/10], Batch [2950/20508], Loss: 0.9249\n",
      "Epoch [1/10], Batch [2960/20508], Loss: 0.8568\n",
      "Epoch [1/10], Batch [2970/20508], Loss: 0.8573\n",
      "Epoch [1/10], Batch [2980/20508], Loss: 0.8275\n",
      "Epoch [1/10], Batch [2990/20508], Loss: 0.8675\n",
      "Epoch [1/10], Batch [3000/20508], Loss: 0.7173\n",
      "Epoch [1/10], Batch [3010/20508], Loss: 0.9064\n",
      "Epoch [1/10], Batch [3020/20508], Loss: 0.9700\n",
      "Epoch [1/10], Batch [3030/20508], Loss: 0.8296\n",
      "Epoch [1/10], Batch [3040/20508], Loss: 0.7941\n",
      "Epoch [1/10], Batch [3050/20508], Loss: 0.7124\n",
      "Epoch [1/10], Batch [3060/20508], Loss: 0.9397\n",
      "Epoch [1/10], Batch [3070/20508], Loss: 0.8495\n",
      "Epoch [1/10], Batch [3080/20508], Loss: 0.8489\n",
      "Epoch [1/10], Batch [3090/20508], Loss: 0.9381\n",
      "Epoch [1/10], Batch [3100/20508], Loss: 0.8711\n",
      "Epoch [1/10], Batch [3110/20508], Loss: 0.7822\n",
      "Epoch [1/10], Batch [3120/20508], Loss: 0.8576\n",
      "Epoch [1/10], Batch [3130/20508], Loss: 0.7523\n",
      "Epoch [1/10], Batch [3140/20508], Loss: 0.8318\n",
      "Epoch [1/10], Batch [3150/20508], Loss: 0.8601\n",
      "Epoch [1/10], Batch [3160/20508], Loss: 0.8136\n",
      "Epoch [1/10], Batch [3170/20508], Loss: 0.7868\n",
      "Epoch [1/10], Batch [3180/20508], Loss: 0.8505\n",
      "Epoch [1/10], Batch [3190/20508], Loss: 0.8745\n",
      "Epoch [1/10], Batch [3200/20508], Loss: 0.8000\n",
      "Epoch [1/10], Batch [3210/20508], Loss: 0.8726\n",
      "Epoch [1/10], Batch [3220/20508], Loss: 0.8272\n",
      "Epoch [1/10], Batch [3230/20508], Loss: 0.8523\n",
      "Epoch [1/10], Batch [3240/20508], Loss: 1.0055\n",
      "Epoch [1/10], Batch [3250/20508], Loss: 0.8606\n",
      "Epoch [1/10], Batch [3260/20508], Loss: 0.7954\n",
      "Epoch [1/10], Batch [3270/20508], Loss: 0.8800\n",
      "Epoch [1/10], Batch [3280/20508], Loss: 0.8034\n",
      "Epoch [1/10], Batch [3290/20508], Loss: 0.8615\n",
      "Epoch [1/10], Batch [3300/20508], Loss: 1.0272\n",
      "Epoch [1/10], Batch [3310/20508], Loss: 0.7900\n",
      "Epoch [1/10], Batch [3320/20508], Loss: 0.8829\n",
      "Epoch [1/10], Batch [3330/20508], Loss: 0.8861\n",
      "Epoch [1/10], Batch [3340/20508], Loss: 0.8396\n",
      "Epoch [1/10], Batch [3350/20508], Loss: 0.9365\n",
      "Epoch [1/10], Batch [3360/20508], Loss: 0.9163\n",
      "Epoch [1/10], Batch [3370/20508], Loss: 0.8936\n",
      "Epoch [1/10], Batch [3380/20508], Loss: 0.9465\n",
      "Epoch [1/10], Batch [3390/20508], Loss: 0.7874\n",
      "Epoch [1/10], Batch [3400/20508], Loss: 0.8616\n",
      "Epoch [1/10], Batch [3410/20508], Loss: 0.7911\n",
      "Epoch [1/10], Batch [3420/20508], Loss: 0.9136\n",
      "Epoch [1/10], Batch [3430/20508], Loss: 0.8407\n",
      "Epoch [1/10], Batch [3440/20508], Loss: 0.8849\n",
      "Epoch [1/10], Batch [3450/20508], Loss: 0.8496\n",
      "Epoch [1/10], Batch [3460/20508], Loss: 0.8061\n",
      "Epoch [1/10], Batch [3470/20508], Loss: 0.9507\n",
      "Epoch [1/10], Batch [3480/20508], Loss: 0.8495\n",
      "Epoch [1/10], Batch [3490/20508], Loss: 0.8352\n",
      "Epoch [1/10], Batch [3500/20508], Loss: 0.8430\n",
      "Epoch [1/10], Batch [3510/20508], Loss: 0.9039\n",
      "Epoch [1/10], Batch [3520/20508], Loss: 0.9180\n",
      "Epoch [1/10], Batch [3530/20508], Loss: 0.8668\n",
      "Epoch [1/10], Batch [3540/20508], Loss: 0.8257\n",
      "Epoch [1/10], Batch [3550/20508], Loss: 0.7604\n",
      "Epoch [1/10], Batch [3560/20508], Loss: 0.9531\n",
      "Epoch [1/10], Batch [3570/20508], Loss: 0.8398\n",
      "Epoch [1/10], Batch [3580/20508], Loss: 0.9126\n",
      "Epoch [1/10], Batch [3590/20508], Loss: 0.8397\n",
      "Epoch [1/10], Batch [3600/20508], Loss: 0.8088\n",
      "Epoch [1/10], Batch [3610/20508], Loss: 0.8980\n",
      "Epoch [1/10], Batch [3620/20508], Loss: 0.7769\n",
      "Epoch [1/10], Batch [3630/20508], Loss: 0.8298\n",
      "Epoch [1/10], Batch [3640/20508], Loss: 0.7195\n",
      "Epoch [1/10], Batch [3650/20508], Loss: 0.9097\n",
      "Epoch [1/10], Batch [3660/20508], Loss: 0.8689\n",
      "Epoch [1/10], Batch [3670/20508], Loss: 0.9196\n",
      "Epoch [1/10], Batch [3680/20508], Loss: 0.8539\n",
      "Epoch [1/10], Batch [3690/20508], Loss: 0.7582\n",
      "Epoch [1/10], Batch [3700/20508], Loss: 0.8846\n",
      "Epoch [1/10], Batch [3710/20508], Loss: 0.7948\n",
      "Epoch [1/10], Batch [3720/20508], Loss: 0.9546\n",
      "Epoch [1/10], Batch [3730/20508], Loss: 0.8972\n",
      "Epoch [1/10], Batch [3740/20508], Loss: 0.8813\n",
      "Epoch [1/10], Batch [3750/20508], Loss: 0.9202\n",
      "Epoch [1/10], Batch [3760/20508], Loss: 0.8919\n",
      "Epoch [1/10], Batch [3770/20508], Loss: 0.8499\n",
      "Epoch [1/10], Batch [3780/20508], Loss: 0.8170\n",
      "Epoch [1/10], Batch [3790/20508], Loss: 0.9849\n",
      "Epoch [1/10], Batch [3800/20508], Loss: 0.9621\n",
      "Epoch [1/10], Batch [3810/20508], Loss: 0.7408\n",
      "Epoch [1/10], Batch [3820/20508], Loss: 0.8204\n",
      "Epoch [1/10], Batch [3830/20508], Loss: 0.9049\n",
      "Epoch [1/10], Batch [3840/20508], Loss: 0.8235\n",
      "Epoch [1/10], Batch [3850/20508], Loss: 0.8637\n",
      "Epoch [1/10], Batch [3860/20508], Loss: 0.9245\n",
      "Epoch [1/10], Batch [3870/20508], Loss: 0.8587\n",
      "Epoch [1/10], Batch [3880/20508], Loss: 0.7788\n",
      "Epoch [1/10], Batch [3890/20508], Loss: 0.7604\n",
      "Epoch [1/10], Batch [3900/20508], Loss: 0.8584\n",
      "Epoch [1/10], Batch [3910/20508], Loss: 0.9548\n",
      "Epoch [1/10], Batch [3920/20508], Loss: 0.8079\n",
      "Epoch [1/10], Batch [3930/20508], Loss: 0.8384\n",
      "Epoch [1/10], Batch [3940/20508], Loss: 0.7771\n",
      "Epoch [1/10], Batch [3950/20508], Loss: 0.9187\n",
      "Epoch [1/10], Batch [3960/20508], Loss: 0.8949\n",
      "Epoch [1/10], Batch [3970/20508], Loss: 0.8545\n",
      "Epoch [1/10], Batch [3980/20508], Loss: 0.8474\n",
      "Epoch [1/10], Batch [3990/20508], Loss: 0.7995\n",
      "Epoch [1/10], Batch [4000/20508], Loss: 0.9403\n",
      "Epoch [1/10], Batch [4010/20508], Loss: 0.8484\n",
      "Epoch [1/10], Batch [4020/20508], Loss: 0.7373\n",
      "Epoch [1/10], Batch [4030/20508], Loss: 0.8778\n",
      "Epoch [1/10], Batch [4040/20508], Loss: 0.7747\n",
      "Epoch [1/10], Batch [4050/20508], Loss: 0.7697\n",
      "Epoch [1/10], Batch [4060/20508], Loss: 0.8326\n",
      "Epoch [1/10], Batch [4070/20508], Loss: 0.7656\n",
      "Epoch [1/10], Batch [4080/20508], Loss: 0.9080\n",
      "Epoch [1/10], Batch [4090/20508], Loss: 0.9092\n",
      "Epoch [1/10], Batch [4100/20508], Loss: 0.8225\n",
      "Epoch [1/10], Batch [4110/20508], Loss: 0.8884\n",
      "Epoch [1/10], Batch [4120/20508], Loss: 0.8625\n",
      "Epoch [1/10], Batch [4130/20508], Loss: 0.8520\n",
      "Epoch [1/10], Batch [4140/20508], Loss: 0.8438\n",
      "Epoch [1/10], Batch [4150/20508], Loss: 0.8265\n",
      "Epoch [1/10], Batch [4160/20508], Loss: 0.7476\n",
      "Epoch [1/10], Batch [4170/20508], Loss: 0.8808\n",
      "Epoch [1/10], Batch [4180/20508], Loss: 0.8387\n",
      "Epoch [1/10], Batch [4190/20508], Loss: 0.8982\n",
      "Epoch [1/10], Batch [4200/20508], Loss: 0.8516\n",
      "Epoch [1/10], Batch [4210/20508], Loss: 0.9176\n",
      "Epoch [1/10], Batch [4220/20508], Loss: 0.9022\n",
      "Epoch [1/10], Batch [4230/20508], Loss: 0.8293\n",
      "Epoch [1/10], Batch [4240/20508], Loss: 0.7818\n",
      "Epoch [1/10], Batch [4250/20508], Loss: 0.7983\n",
      "Epoch [1/10], Batch [4260/20508], Loss: 0.8702\n",
      "Epoch [1/10], Batch [4270/20508], Loss: 0.8817\n",
      "Epoch [1/10], Batch [4280/20508], Loss: 0.8191\n",
      "Epoch [1/10], Batch [4290/20508], Loss: 0.8652\n",
      "Epoch [1/10], Batch [4300/20508], Loss: 0.9039\n",
      "Epoch [1/10], Batch [4310/20508], Loss: 0.9193\n",
      "Epoch [1/10], Batch [4320/20508], Loss: 0.8808\n",
      "Epoch [1/10], Batch [4330/20508], Loss: 0.8881\n",
      "Epoch [1/10], Batch [4340/20508], Loss: 0.7617\n",
      "Epoch [1/10], Batch [4350/20508], Loss: 0.9112\n",
      "Epoch [1/10], Batch [4360/20508], Loss: 0.7729\n",
      "Epoch [1/10], Batch [4370/20508], Loss: 0.8400\n",
      "Epoch [1/10], Batch [4380/20508], Loss: 0.9431\n",
      "Epoch [1/10], Batch [4390/20508], Loss: 0.9139\n",
      "Epoch [1/10], Batch [4400/20508], Loss: 0.7453\n",
      "Epoch [1/10], Batch [4410/20508], Loss: 0.8994\n",
      "Epoch [1/10], Batch [4420/20508], Loss: 0.8592\n",
      "Epoch [1/10], Batch [4430/20508], Loss: 0.9031\n",
      "Epoch [1/10], Batch [4440/20508], Loss: 0.8980\n",
      "Epoch [1/10], Batch [4450/20508], Loss: 0.8817\n",
      "Epoch [1/10], Batch [4460/20508], Loss: 0.7636\n",
      "Epoch [1/10], Batch [4470/20508], Loss: 0.8124\n",
      "Epoch [1/10], Batch [4480/20508], Loss: 0.8585\n",
      "Epoch [1/10], Batch [4490/20508], Loss: 0.8108\n",
      "Epoch [1/10], Batch [4500/20508], Loss: 0.8397\n",
      "Epoch [1/10], Batch [4510/20508], Loss: 0.8919\n",
      "Epoch [1/10], Batch [4520/20508], Loss: 0.8445\n",
      "Epoch [1/10], Batch [4530/20508], Loss: 0.7954\n",
      "Epoch [1/10], Batch [4540/20508], Loss: 0.8308\n",
      "Epoch [1/10], Batch [4550/20508], Loss: 0.8993\n",
      "Epoch [1/10], Batch [4560/20508], Loss: 0.8017\n",
      "Epoch [1/10], Batch [4570/20508], Loss: 0.9723\n",
      "Epoch [1/10], Batch [4580/20508], Loss: 0.8691\n",
      "Epoch [1/10], Batch [4590/20508], Loss: 0.8568\n",
      "Epoch [1/10], Batch [4600/20508], Loss: 0.8456\n",
      "Epoch [1/10], Batch [4610/20508], Loss: 0.8157\n",
      "Epoch [1/10], Batch [4620/20508], Loss: 0.8263\n",
      "Epoch [1/10], Batch [4630/20508], Loss: 0.8721\n",
      "Epoch [1/10], Batch [4640/20508], Loss: 0.8123\n",
      "Epoch [1/10], Batch [4650/20508], Loss: 0.8622\n",
      "Epoch [1/10], Batch [4660/20508], Loss: 0.7506\n",
      "Epoch [1/10], Batch [4670/20508], Loss: 0.8388\n",
      "Epoch [1/10], Batch [4680/20508], Loss: 0.8176\n",
      "Epoch [1/10], Batch [4690/20508], Loss: 0.8620\n",
      "Epoch [1/10], Batch [4700/20508], Loss: 0.8458\n",
      "Epoch [1/10], Batch [4710/20508], Loss: 0.8471\n",
      "Epoch [1/10], Batch [4720/20508], Loss: 0.7416\n",
      "Epoch [1/10], Batch [4730/20508], Loss: 0.8048\n",
      "Epoch [1/10], Batch [4740/20508], Loss: 0.8255\n",
      "Epoch [1/10], Batch [4750/20508], Loss: 0.8785\n",
      "Epoch [1/10], Batch [4760/20508], Loss: 0.8778\n",
      "Epoch [1/10], Batch [4770/20508], Loss: 0.8801\n",
      "Epoch [1/10], Batch [4780/20508], Loss: 0.8057\n",
      "Epoch [1/10], Batch [4790/20508], Loss: 0.9677\n",
      "Epoch [1/10], Batch [4800/20508], Loss: 0.7780\n",
      "Epoch [1/10], Batch [4810/20508], Loss: 0.8022\n",
      "Epoch [1/10], Batch [4820/20508], Loss: 0.8664\n",
      "Epoch [1/10], Batch [4830/20508], Loss: 0.8056\n",
      "Epoch [1/10], Batch [4840/20508], Loss: 0.9990\n",
      "Epoch [1/10], Batch [4850/20508], Loss: 0.7938\n",
      "Epoch [1/10], Batch [4860/20508], Loss: 0.8238\n",
      "Epoch [1/10], Batch [4870/20508], Loss: 0.7969\n",
      "Epoch [1/10], Batch [4880/20508], Loss: 0.8088\n",
      "Epoch [1/10], Batch [4890/20508], Loss: 0.8577\n",
      "Epoch [1/10], Batch [4900/20508], Loss: 0.7692\n",
      "Epoch [1/10], Batch [4910/20508], Loss: 0.8250\n",
      "Epoch [1/10], Batch [4920/20508], Loss: 0.7727\n",
      "Epoch [1/10], Batch [4930/20508], Loss: 0.8534\n",
      "Epoch [1/10], Batch [4940/20508], Loss: 0.7956\n",
      "Epoch [1/10], Batch [4950/20508], Loss: 0.8509\n",
      "Epoch [1/10], Batch [4960/20508], Loss: 0.8130\n",
      "Epoch [1/10], Batch [4970/20508], Loss: 0.8339\n",
      "Epoch [1/10], Batch [4980/20508], Loss: 0.7726\n",
      "Epoch [1/10], Batch [4990/20508], Loss: 0.8674\n",
      "Epoch [1/10], Batch [5000/20508], Loss: 0.7130\n",
      "Epoch [1/10], Batch [5010/20508], Loss: 0.7170\n",
      "Epoch [1/10], Batch [5020/20508], Loss: 0.7886\n",
      "Epoch [1/10], Batch [5030/20508], Loss: 0.8184\n",
      "Epoch [1/10], Batch [5040/20508], Loss: 0.8547\n",
      "Epoch [1/10], Batch [5050/20508], Loss: 0.8247\n",
      "Epoch [1/10], Batch [5060/20508], Loss: 0.6892\n",
      "Epoch [1/10], Batch [5070/20508], Loss: 0.8299\n",
      "Epoch [1/10], Batch [5080/20508], Loss: 0.7428\n",
      "Epoch [1/10], Batch [5090/20508], Loss: 0.8391\n",
      "Epoch [1/10], Batch [5100/20508], Loss: 0.8850\n",
      "Epoch [1/10], Batch [5110/20508], Loss: 0.8301\n",
      "Epoch [1/10], Batch [5120/20508], Loss: 0.8666\n",
      "Epoch [1/10], Batch [5130/20508], Loss: 0.8691\n",
      "Epoch [1/10], Batch [5140/20508], Loss: 0.7599\n",
      "Epoch [1/10], Batch [5150/20508], Loss: 0.7351\n",
      "Epoch [1/10], Batch [5160/20508], Loss: 0.7971\n",
      "Epoch [1/10], Batch [5170/20508], Loss: 0.8390\n",
      "Epoch [1/10], Batch [5180/20508], Loss: 0.7195\n",
      "Epoch [1/10], Batch [5190/20508], Loss: 0.8951\n",
      "Epoch [1/10], Batch [5200/20508], Loss: 0.8582\n",
      "Epoch [1/10], Batch [5210/20508], Loss: 0.8341\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        img_dir = 'C:\\\\Users\\\\680-9000\\\\Desktop\\\\finance\\\\datav3\\\\Image1'\n",
    "        labels_file = 'C:\\\\Users\\\\680-9000\\\\Desktop\\\\finance\\\\datav3\\\\X_5_5.xlsx'\n",
    "        \n",
    "        print(\"正在載入數據...\")\n",
    "        dataset = CustomImageDataset(img_dir=img_dir, labels_file=labels_file)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size]) if use_random_split else (dataset, dataset)\n",
    "        \n",
    "        # train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "        print(\"初始化模型...\")\n",
    "        model = CNN5d()\n",
    "        if use_dataparallel and torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        print(\"開始訓練...\")\n",
    "        train_losses, test_losses = train_model(model, train_loader, test_loader)\n",
    "        \n",
    "        print(\"評估模型...\")\n",
    "        accuracy, conf_matrix = evaluate_model(model, test_loader)\n",
    "        print(f\"\\nFinal Test Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"GPU 記憶體不足！嘗試減少 batch size 或圖片大小。\")\n",
    "        else:\n",
    "            print(f\"運行時錯誤: {str(e)}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(f\"發生錯誤: {str(e)}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
